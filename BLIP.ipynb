{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "YQk0eemUrSC7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-8c5f31c0-aec4-5828-1fc6-24bd25119294)\n",
      "GPU 1: NVIDIA GeForce RTX 3090 (UUID: GPU-99ffc975-a6e6-3d71-c76b-0fc2d35723ad)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "30xPxDSDrJEl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arty/clip-interrogator/BLIP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arty/miniconda3/envs/camus/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth\n"
     ]
    }
   ],
   "source": [
    "#@title Setup\n",
    "%cd ./BLIP\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from models.blip import blip_decoder\n",
    "\n",
    "# Set device\n",
    "device = 'cuda'\n",
    "\n",
    "# Set constants\n",
    "size = 384\n",
    "url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'\n",
    "\n",
    "# Load BLIP model\n",
    "blip = blip_decoder(pretrained=url, image_size=size, vit='base')\n",
    "blip.eval()\n",
    "blip = blip.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(pil_image):\n",
    "    # Preprocess image\n",
    "    input_image = T.Compose([\n",
    "        T.Resize((size, size), interpolation=TF.InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        caption = blip.generate(input_image, sample=False, num_beams=3, max_length=1000, min_length=100)\n",
    "        \n",
    "    return caption[0]\n",
    "\n",
    "def interrogate(image, models=[]):\n",
    "    # Generate caption\n",
    "    caption = generate_caption(image)\n",
    "    print(f\"\\n\\n{caption}\")\n",
    "    \n",
    "    # If no models are provided, return\n",
    "    if not models:\n",
    "        return\n",
    "    \n",
    "    # Set up bests and flaves\n",
    "    bests = [[('',0)]]*5\n",
    "    flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
    "    \n",
    "    # Set up medium\n",
    "    medium = bests[0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"../BLIP/11.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViTB32 = True\n",
    "models = []\n",
    "if ViTB32: models.append('ViT-B/32')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "rbDEMDGJrJEo",
    "outputId": "dd9742e7-1d09-4684-9a00-c1a3284513a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "a computer generated image of a bathroom with a sink, mirror, and a toaster on the counter top, and a television mounted on the wall above the bathtub in the corner of the room is a black tiled area with a wooden floor, and a wooden floor, a window, and a window, and a window, and a window, and a window,???????????????????????\n"
     ]
    }
   ],
   "source": [
    "interrogate(image, models=models)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "clip-interrogator.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.4 ('camus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f929cfddc8e2c0b26c670ccb4919da1781bcc4617ac2e12dfdc86faab5346303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
